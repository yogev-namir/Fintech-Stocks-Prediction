{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium bs4 beautifulsoup4 pandas webdriver_manager > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topic_url\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Get topic from the user\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter the topic to scrape: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m url \u001b[38;5;241m=\u001b[39m get_topic_url(topic)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Set up conditions\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Function to get topic from the user\n",
    "def get_topic_url(topic):\n",
    "    base_url = \"https://economictimes.indiatimes.com/topic/\"\n",
    "    topic_url = base_url + topic.replace(\" \", \"-\") + \"/news\"\n",
    "    return topic_url\n",
    "\n",
    "# Get topic from the user\n",
    "topic = input(\"Enter the topic to scrape: \")\n",
    "url = get_topic_url(topic)\n",
    "\n",
    "# Set up conditions\n",
    "max_articles = input(\"Enter the maximum number of articles to scrape (leave blank for no limit): \")\n",
    "max_articles = int(max_articles) if max_articles else None\n",
    "\n",
    "max_date_str = input(\"Enter the latest date to scrape articles (in format YYYY-MM-DD, leave blank for no limit): \")\n",
    "max_date = datetime.strptime(max_date_str, \"%Y-%m-%d\") if max_date_str else None\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "driver.get(url)\n",
    "\n",
    "# Initialize a list to store the scraped data\n",
    "data = []\n",
    "scraped_links = set()  # To keep track of scraped links\n",
    "\n",
    "def extract_articles(articles):\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Find the title and link\n",
    "            title_tag = article.find(\"h2\")\n",
    "            if title_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                link_tag = article.find(\"a\")\n",
    "                link = link_tag[\"href\"] if link_tag else None\n",
    "            else:\n",
    "                # Handle articles in the container\n",
    "                link_tag = article.find(\"a\", itemprop=\"url\")\n",
    "                if not link_tag:\n",
    "                    continue\n",
    "                title = link_tag.get(\"title\", link_tag.find(\"meta\", itemprop=\"name\")[\"content\"])\n",
    "                link = link_tag[\"href\"]\n",
    "            \n",
    "            if not link.startswith(\"http\"):\n",
    "                link = \"https://economictimes.indiatimes.com\" + link\n",
    "\n",
    "            # Skip if the article has already been scraped\n",
    "            if link in scraped_links:\n",
    "                continue\n",
    "            scraped_links.add(link)\n",
    "\n",
    "            # Find the description\n",
    "            description_tag = article.find(\"p\", class_=\"wrapLines l3\") or article.find(\"div\", class_=\"wrapLines l4\")\n",
    "            description = description_tag.get_text(strip=True) if description_tag else \"No description\"\n",
    "\n",
    "            # Find the date of publication\n",
    "            time_tag = article.find(\"time\")\n",
    "            time_value = time_tag.get_text(strip=True) if time_tag else \"No time\"\n",
    "            date_parts = time_value.split(\",\")[:2]  # Get the date part only (day, month, year)\n",
    "            date_value = \",\".join(date_parts).strip()\n",
    "\n",
    "            article_date = datetime.strptime(date_value, \"%d %b, %Y\") if time_tag else None\n",
    "            formatted_date = article_date.strftime(\"%Y-%m-%d\") if article_date else \"No date\"\n",
    "\n",
    "            # Append the extracted data to the list\n",
    "            data.append({\n",
    "                \"Title\": title,\n",
    "                \"Link\": link,\n",
    "                \"Description\": description,\n",
    "                \"Date\": formatted_date\n",
    "            })\n",
    "\n",
    "            # Check the conditions\n",
    "            if max_articles and len(data) >= max_articles:\n",
    "                return True\n",
    "            if max_date and article_date and article_date < max_date:\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article: {e}\")\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "try:\n",
    "    # Initial parse to get the first set of articles\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    initial_articles = soup.find_all(\"div\", class_=\"clr flt topicstry story_list\")\n",
    "    print(f\"Found {len(initial_articles)} initial articles.\")\n",
    "    if extract_articles(initial_articles):\n",
    "        raise StopIteration\n",
    "\n",
    "    # Scroll and click \"Load More\" until no more content is loaded\n",
    "    while True:\n",
    "        # Scroll all the way to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # wait for the page to load new content\n",
    "\n",
    "        # Try to click the \"Load More\" button using JavaScript\n",
    "        try:\n",
    "            load_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'jsTopicLoadMore'))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "            time.sleep(3)  # wait for new content to load\n",
    "        except:\n",
    "            print(\"No more 'Load More' button found or it could not be clicked. Stopping the scroll.\")\n",
    "            break  # no more \"Load More\" button, exit loop\n",
    "\n",
    "        # Parse the new content\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        # Find all new article containers\n",
    "        more_stories = soup.find_all(\"div\", class_=\"moreStories\")\n",
    "        print(f\"Found {len(more_stories)} 'moreStories' containers on the current page.\")\n",
    "\n",
    "        # Extract data from each article element within new containers\n",
    "        for container in more_stories:\n",
    "            articles = container.find_all(\"div\", class_=\"clr flt topicstry\")\n",
    "            print(f\"Found {len(articles)} articles in the current 'moreStories' container.\")\n",
    "            if extract_articles(articles):\n",
    "                raise StopIteration\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"Stopping the scraping process as conditions were met.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    # Convert the data into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df.head())\n",
    "\n",
    "    # Construct the filename\n",
    "    filename = f\"{topic.replace(' ', '_')}_articles\"\n",
    "    if max_articles:\n",
    "        filename += f\"_max{max_articles}\"\n",
    "    if max_date:\n",
    "        filename += f\"_end{max_date.strftime('%Y%m%d')}\"\n",
    "    filename += \".csv\"\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    save_path = os.path.join(\"datasets\", \"scrapped_articles\", filename)\n",
    "    df.to_csv(save_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
